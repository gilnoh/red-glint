This memo outlines generalized higher order representation making for
structured vectors (like Distributional Memory, or dependency vectors,
etc).  

* Generalization of 2nd order vector as matrix operations. 
** Case one: element - feature has same items (& dimensions) 
- Let's first define matrix operations on simple word-word matrix
  (word as element, and words as features). Then, extend this into
  dissimilar element-feature casees. 
- A simple word-word matrix example. 

    a   b   c   d  
a   2   2   0   0 
b   2   2   1   0 
c   0   1   1   1 
d   0   0   1   1 

word a - b : heavy relation. 
word b-c. c-d : minor relations
word a - d : no shared feature (sim(a,d)==0) 

- (weighted) 2nd order vector of a would be 

a_2nd =  2*a + 2*b = (4,4,0,0) + (4, 4, 2, 0) = 8, 8, 2, 0
b_2nd =  2*a + 2*b + 1*c = (4,4,0,0) + (4,4,2,0) + (1,0,1,1) = 8,9,3,1 
... 

Actually, this is the matrix square operation. 
octave> b = a * a'
ans =

   8   8   2   0
   8   9   3   1
   2   3   3   2
   0   1   2   2

The value got higher due to multiplications: If normalized (any
normalization would do, but let's do per-row normalization for now.) 

octave:37> b ./ sum(b')'
ans =

   0.44444   0.44444   0.11111   0.00000
   0.38095   0.42857   0.14286   0.04762
   0.20000   0.30000   0.30000   0.20000
   0.00000   0.20000   0.40000   0.40000

- Previously, original matrix normalized (again per-row) was the followings. 
octave:35> a ./ sum(a')'
ans =
   0.50000   0.50000   0.00000   0.00000
   0.40000   0.40000   0.20000   0.00000
   0.33333   0.00000   0.33333   0.33333
   0.00000   0.00000   0.50000   0.50000

*** We can clearly see the "diffusion" effect of 2nd order vectors. 
- a now has a portion of feature C (diffused/smoothed via B) 
- d now has a portion of feature B (diffused via C) 
- b also has a tiny portion of D (diffused via C) 
- And finally, sim(a,d) is no longer 0. 

*** Actually, this is a "special" case, where row ID & column ID are same. 
- In normal case (e.g. different IDs for column/row) what happenes? 
- Let's generalize this into different element - feature space. 

** Case two: what if feature & elements are different? 
(Baroni's exmaple, mode-1 projection of DM.) 

          <own,bomb> <use,bomb> <own,gun>, <use,gun> <own,book> <use,book>
marine     40.0       82.1       85.3       44.8      3.2        3.3 
sergeant   16.7       69.5       73.4       51.9      8.0       10.1 
teacher    5.2         7.0        9.3        4.7     45.4       53.6 

- Recall that our basic idea was that; we first need to have one
  "feature-feature" matrix representation. 
- Once we have "feature-feature" matrix; then generating 2nd order
  vector can be very simple. \Sigma_features ( weight_for_the_feature *
  feature_representation ). 
- But how we get this "feature-feature" matrix? Observing from corpus?
  It might ... work. But not possible in many cases (like above <own,
  bomb>, <own, gun>). 
- Also, we have a much simple, elegant way to generate it, that is
  symmetric to word-word case. 

The example in octave: 

A =
   40.0000   82.1000   85.3000   44.8000    3.2000    3.3000
   16.7000   69.5000   73.4000   51.9000    8.0000   10.1000
    5.2000    7.0000    9.3000    4.7000   45.4000   53.6000

octave:40> B = A'*A
ans =

   1.9059e+03   4.4810e+03   4.6861e+03   2.6832e+03   4.9768e+02   5.7939e+02
   4.4810e+03   1.1620e+04   1.2170e+04   7.3180e+03   1.1365e+03   1.3481e+03
   4.6861e+03   1.2170e+04   1.2750e+04   7.6746e+03   1.2824e+03   1.5213e+03
   2.6832e+03   7.3180e+03   7.6746e+03   4.7227e+03   7.7194e+02   9.2395e+02
   4.9768e+02   1.1365e+03   1.2824e+03   7.7194e+02   2.1354e+03   2.5248e+03
   5.7939e+02   1.3481e+03   1.5213e+03   9.2395e+02   2.5248e+03   2.9859e+03

- The matrix is a square matrix of number of features. Let's call it
  B matrix. 
- B(1,2) indicates count for <own, bomb> -  <use, bomb>: this is
  calculated by summing up multiplication of their shared
  "elements". (inner product of element vectors associated to the two
  features). 
- For example, B(1,2) was calculated by 40.0 * 82.1 + 16.7 * 69.5 +
  5.2 * 7.0 = 4481 
- Note that this is actually the innerproduct of <own,bomb> vector and
  <use, bomb> vector (where the vector dimension is the elements:
  marine, sergeant, teacher).  
- The table can serve as "feature - feature" table. Each cell
  represents "inner-product" between features, according to their
  "observed" counts on elements. 
- Now we have the feature-to-feature table: we can use it to generate
  the real thing that we wanted: 2nd-order vector of elements. 
- Recall that 2nd-order vector element = (for each feature) feature
  weight * (feature representation)  
- This is A * B' --- how convenient to have it simply as one matrix
  operation! 

(Unnormalized 2nd-order vector) 
octave:67> A * B'
ans =

   9.6757e+05   2.5072e+06   2.6271e+06   1.5799e+06   2.7235e+05   3.2295e+05
   8.3631e+05   2.1782e+06   2.2838e+06   1.3773e+06   2.6407e+05   3.1334e+05
   1.5112e+05   3.7607e+05   4.0396e+05   2.4332e+05   2.5837e+05   3.0561e+05

- The value is too big. So let's normalize it and compare it with the
original matrix. (again, per-row normalization.) (I've added index
names on the following two examples) 

octave:69> C ./ sum(C')'
warning: quotient: automatic broadcasting operation applied
ans =

           <own,bomb> <use,bomb> <own,gun>  <use,gun>  <own,book> <use,book>
marine     0.116898   0.302911   0.317395   0.190875   0.032904   0.039017
sergeant   0.115305   0.300308   0.314879   0.189898   0.036409   0.043201
teacher    0.086928   0.216322   0.232370   0.139963   0.148623   0.175793

- Compare this matrix with original matrix. (original matrix with row
  normalized)  
marine     0.154619   0.317356   0.329726   0.173174   0.012370   0.012756
sergeant   0.072735   0.302700   0.319686   0.226045   0.034843   0.043990
teacher    0.041534   0.055911   0.074281   0.037540   0.362620   0.428115

*** Again, we can clearly see the diffusion effect. 
- The upper (diffused) matrix is smoothed. 
- Values are "diffused" and "flowed" into other parts : smoothed,
  according to their "shared number" of elements. 
- sergeant now owns more bomb (cz highly shared amount of behavior to
  marine), and marine reads more book (for US marine, this is very
  unlikely, but maybe sergeants can affect them some how... as seen in
  this 2nd-order. ) 

*** Conclusion of this memo: 
- This is a nice generalization of "higer-order" representations via
  very simple matrix operations. 
- Note that here, what we actually did for DM has has one more
  operation (higher) than what we did for simple word-word matrix. 
  + For simple word matrix, matrix squaring (A*A') and the
    normalization was all that was needed. 
  + For non-symmetric element-feature matrix, what we did is A *
    (A*A')', and then, normalization. 
  + The same operation on word-matrix would mean 3rd-order
    representations.
  + However, I wouldn't call this as 3rd-order representation, becase
    ... 
- To generate 3rd, or even higher order, we can do: 
  X * B' where X: current matrix. (Diffusion matrix B, maynot need to
  be updated all the time.)  

